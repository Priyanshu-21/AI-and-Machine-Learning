{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb1355c2",
   "metadata": {},
   "source": [
    "### Introduction to forward propagation\n",
    "1. Implement from scratch a simple artificial neural network system \n",
    "2. Using only numpy to generate neural network from scratch "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2fa2ee",
   "metadata": {},
   "source": [
    "### Structure of neural network \n",
    "- Taking a small neural network with below features \n",
    "    - 1 input layer with 2 inputs \n",
    "    - 1 hidden layer with 2 inputs [work as ouput of input layer as input]\n",
    "    - 1 neuron for output layer \n",
    "\n",
    "![Image.png](../Img/simple_nueral_network.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92d2535d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.99 0.4  0.16 0.06 0.02 0.47]\n",
      "[0.94 0.08 0.2 ]\n"
     ]
    }
   ],
   "source": [
    "#Importing libraries \n",
    "import numpy as np \n",
    "\n",
    "weights = np.around(np.random.uniform(size= 6), decimals= 2)\n",
    "bias = np.around(np.random.uniform(size= 3), decimals= 2)\n",
    "\n",
    "print(weights)\n",
    "print(bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc08a1a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1 is 0.5 and x2 is 0.85\n"
     ]
    }
   ],
   "source": [
    "#Input node \n",
    "x1 = 0.5\n",
    "x2 = 0.85\n",
    "\n",
    "print('x1 is {} and x2 is {}'.format(x1, x2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00a89fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate z for the hidden and ouput layer \n",
    "\n",
    "#Hidden Layer 1 \n",
    "z11 = x1 * weights[0] + bias[0]\n",
    "z12 = x1 * weights[1] + bias[1]\n",
    "\n",
    "#Hidden Layer 2 \n",
    "z21 = x2 * weights[2] + bias[0]\n",
    "z22 = x2 * weights[3] + bias[1]\n",
    "\n",
    "#Applying activation function (using Sigmoid Function)\n",
    "sigma = lambda z : 1 / 1 + np.exp(-z)\n",
    "\n",
    "#Activation function at each layer \n",
    "z11 = sigma(z11)\n",
    "z12 = sigma(z12)\n",
    "z21 = sigma(z21)\n",
    "z22 = sigma(z22)\n",
    "\n",
    "#Output layer \n",
    "z3 = (z11 + z21) * weights[4] + bias[2] \n",
    "z4 = (z12 + z22) * weights[5] + bias[2]\n",
    "\n",
    "z3 = sigma(z3)\n",
    "z4 = sigma(z4)\n",
    "\n",
    "#Actual output \n",
    "z = z3 + z4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97b62a68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output of simple neural network is 2.93\n"
     ]
    }
   ],
   "source": [
    "print('The output of simple neural network is {}'.format(np.round(z, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34bbbaa",
   "metadata": {},
   "source": [
    "### Introduction to General Neural Network \n",
    "1. Input layer can be derived to a input matrix, having collection of int/float points `$X_1`\n",
    "2. Hidden layer can work as an intermediate layer to help the nueual netork to find pattern and get us back to optimal output. \n",
    "    - This layer help's us to find the interconnection and relationship between input and output layer. \n",
    "3. Output layer is also a matrix having collecton of homogeneous int/ float numbers \n",
    "\n",
    "- Mathematically, \n",
    "    - $Z = X.W + B$\n",
    "    - $A = F(Z)$\n",
    "\n",
    "- Structure of General Neural network will look like: - \n",
    "![Image.png](../Img/General_neural_network.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12245de",
   "metadata": {},
   "source": [
    "### Code Structure of the General Neural Network \n",
    "1. Have build the structure of the neural network \n",
    "2. Initialize the values of each layer using standard random function \n",
    "3. Calculate outputs at each node in forward propagation \n",
    "4. Having activate the activation function for each output calculation \n",
    "5. Make output prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23ddd701",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize the structure of neural netowrk \n",
    "#Input layer \n",
    "input_layer = 2\n",
    "\n",
    "#Hidden layer \n",
    "hidden_layer = 2\n",
    "\n",
    "#Node's in each hidden layer \n",
    "m = [2, 2]\n",
    "\n",
    "#Output layer\n",
    "output_layer = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c17209a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determine the structure \n",
    "def initialize_network(input_layer, hidden_layer, num_hidden_nodes, output_layer):\n",
    "    #Previous layer \n",
    "    num_previous_layer = input_layer\n",
    "    #Dictionary to store network layer information \n",
    "    network = {}\n",
    "\n",
    "    #In each node, having the weights and bias associated \n",
    "    #Traversing from hidden layer 1 to Output layer \n",
    "    for layer in range(hidden_layer + 1):\n",
    "\n",
    "        #Output layer \n",
    "        if layer == hidden_layer:\n",
    "            layer_name = 'output'\n",
    "            num_nodes = output_layer\n",
    "        else:\n",
    "            layer_name = 'layer_{}'.format(layer) #Hidden layer value\n",
    "            num_nodes = num_hidden_nodes[layer]\n",
    "\n",
    "        #Assigning each neuron it's weight and bias \n",
    "        network[layer_name] = {}\n",
    "        for node in range(num_nodes):\n",
    "            node_name = 'node_{}'.format(node + 1) #Node name in the layer\n",
    "            network[layer_name][node_name] = {\n",
    "                'weights': np.around(np.random.uniform(size= num_previous_layer), decimals= 2),\n",
    "                'bias': np.around(np.random.uniform(size= 1), decimals= 2)\n",
    "            }\n",
    "        num_previous_layer = num_nodes\n",
    "    \n",
    "    return network \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8536b4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "network = initialize_network(input_layer, hidden_layer, m, output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e1ebcd40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'layer_0': {'node_1': {'weights': array([0.73, 0.7 ]), 'bias': array([0.33])}, 'node_2': {'weights': array([0.33, 0.98]), 'bias': array([0.62])}}, 'layer_1': {'node_1': {'weights': array([0.95, 0.77]), 'bias': array([0.83])}, 'node_2': {'weights': array([0.41, 0.45]), 'bias': array([0.4])}}, 'output': {'node_1': {'weights': array([1.  , 0.18]), 'bias': array([0.96])}}}\n"
     ]
    }
   ],
   "source": [
    "print(network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "09dc50a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'layer_0': {'node_1': {'weights': array([0.15, 0.74, 0.26, 0.53, 0.01]),\n",
       "   'bias': array([0.92])},\n",
       "  'node_2': {'weights': array([0.9 , 0.03, 0.96, 0.14, 0.28]),\n",
       "   'bias': array([0.61])},\n",
       "  'node_3': {'weights': array([0.94, 0.85, 0.  , 0.52, 0.55]),\n",
       "   'bias': array([0.49])}},\n",
       " 'layer_1': {'node_1': {'weights': array([0.77, 0.16, 0.76]),\n",
       "   'bias': array([0.02])},\n",
       "  'node_2': {'weights': array([0.14, 0.12, 0.31]), 'bias': array([0.67])}},\n",
       " 'layer_2': {'node_1': {'weights': array([0.47, 0.82]), 'bias': array([0.29])},\n",
       "  'node_2': {'weights': array([0.73, 0.7 ]), 'bias': array([0.33])},\n",
       "  'node_3': {'weights': array([0.33, 0.98]), 'bias': array([0.62])}},\n",
       " 'output': {'node_1': {'weights': array([0.95, 0.77, 0.83]),\n",
       "   'bias': array([0.41])}}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(12)\n",
    "initialize_network(5, 3, [3, 2, 3], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "89af83ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate weights and bias sum\n",
    "def calculate_weighted_sum(inputs, weights, bias):\n",
    "    return np.sum(inputs * weights) + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3c6d55bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calcuate activation function (Activation function used: Sigmoid function)\n",
    "def activation_function(node):\n",
    "    return 1.0 / (1.0 + np.exp(-1 * node))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2930db78",
   "metadata": {},
   "source": [
    "### Feed Forward Algorithm approach \n",
    "\n",
    "The way we are going to accomplish this is through the following procedure:\n",
    "\n",
    "1. Start with the input layer as the input to the first hidden layer.\n",
    "2. Compute the weighted sum at the nodes of the current layer.\n",
    "3. Compute the output of the nodes of the current layer.\n",
    "4. Set the output of the current layer to be the input to the next layer.\n",
    "5. Move to the next layer in the network.\n",
    "6. Repeat steps 2 - 5 until we compute the output of the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dcc3cf20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward(inputs, networks):\n",
    "    #Start with input layer \n",
    "    input_layer = list(inputs)\n",
    "    \n",
    "    for layer in networks:\n",
    "        layer_data = networks[layer]\n",
    "        layer_outputs = [] #To calculate each layer weighted sum and activation \n",
    "        \n",
    "        #Need to traverse each node in specific layer\n",
    "        for layer_node in layer_data:\n",
    "            node_data = layer_data[layer_node]\n",
    "\n",
    "            #Calcuate each node output \n",
    "            node_outputs = activation_function(calculate_weighted_sum(input_layer, node_data['weights'], node_data['bias']))\n",
    "            layer_outputs.append(np.around(node_outputs[0], decimals= 4))\n",
    "\n",
    "        #Print calculation of each layer output\n",
    "        if (layer != 'output'):\n",
    "            print('Output for layer {} is {}'.format(layer.split('_')[1], layer_outputs))\n",
    "        \n",
    "        input_layer = layer_outputs\n",
    "    \n",
    "    network_predictions = layer_outputs\n",
    "    return network_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "15894d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output for layer 0 is [np.float64(0.8323), np.float64(0.8268), np.float64(0.7735)]\n",
      "Output for layer 1 is [np.float64(0.7932), np.float64(0.8991)]\n",
      "Output for layer 2 is [np.float64(0.8232), np.float64(0.8924), np.float64(0.8141)]\n",
      "Prediction of the neural network:  [np.float64(0.9112)]\n"
     ]
    }
   ],
   "source": [
    "#initialize the inputs \n",
    "input_layer = 5\n",
    "np.random.seed(12)\n",
    "\n",
    "inputs = np.around(np.random.uniform(size= input_layer), decimals= 2)\n",
    "network = initialize_network(input_layer, 3, [3, 2, 3], 1)\n",
    "predictions = feed_forward(inputs, network)\n",
    "\n",
    "print(\"Prediction of the neural network: \", predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI-and-Machine-Learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
